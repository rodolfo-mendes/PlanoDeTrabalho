\section{Revisão da Literatura Correlata} \label{sec:fundamentacao_teorica}

Nesta seção apresentaremos os principais conceitos teóricos relacionados ao
trabalho desenvolvido. Na subseção \ref{subsec:descoberta_conhecimento_bd}
apresentaremos o processo de descoberta de conhecimento em banco de dados e
suas principais etapas. 

\subsection{Descoberta de Conhecimento em Bancos de Dados}
	\label{subsec:descoberta_conhecimento_bd}

A Descoberta de Conhecimento em Bancos de Dados, também conhecida pela sigla KDD 
(\emph{Knowledge Discovery in Databases}) é o processo pelo qual dados brutos,
coletados a partir das mais variadas fontes, são processados e transformados em
informações úteis. Por sua vez, estas informações permitem o aprimoramento da
tomada de decisão e até mesmo ampliação do conhecimento científico sobre um
determinado fenômeno \cite{tan2009introducao}.

O processo de KDD envolve desde a aquisição dos dados até a disponibilização do
conhecimento para o usuário final. De acordo com \cite{tan2009introducao},
este processo pode ser descrito pelas seguintes etapas:

\begin{enumerate}
    \item Pré-processamento
    \item Mineração de dados
    \item Pós-processamento
\end{enumerate}

O objetivo da etapa de pré-processamento é preparar os dados que alimentarão a 
etapa de mineração de dados. Nesta etapa, podem ser realizadas uma série de
tarefas que visam aumentar a qualidade dos dados fornecidos à mineração de
dados. Na tarefa de \emph{limpeza dos dados}, são tratados atributos sem valor
definido e ruídos. A tarefa de \emph{integração de dados} consiste
em consolidar fontes de dados de diversos tipos (arquivos de texto, planilhas,
\emph{web-services}, arquivos XML, bancos de dados) em uma única fonte de dados
consolidada, usualmente um \emph{data-warehouse}. A \emph{redução da
dimensionalidade} consiste em diminuir o número de atributos que serão
considerados na mineração de dados. Dentre as principais técnicas podemos citar
PCA (\emph{Principal Component Analysis}) e DWT (\emph{Discrete Wavelets
Transforms}). Por fim, a \emph{redução da numerosidade} busca representar o
conjunto de dados através de um número reduzido de instâncias
\cite{han2011data}.

O reconhecimento de padrões é efetivamente realizado na etapa de mineração de
dados. As tarefas desta etapa são categorizadas de acordo com o conhecimento que
se deseja extrair da base de dados analisada. Na tarefa de mineração de itens
frequentes, deseja-se extrair de um banco de transações quais itens ocorrem
conjuntamente com maior frequência. Na tarefa de classificação, o objetivo é
inferir um modelo a partir do qual seja possível prever à qual classe uma
determinada instância de dados pertence. Por fim, na análise de agrupamentos
deseja-se descobrir a existência de grupos (\emph{clusters}) de dados. Assim, é
preciso que se estabeleça uma \emph{medida de similaridade} entre as instâncias
do banco de dados, de forma que se maximize a similaridade entre instâncias do
mesmo grupo e se minimize a similaridade entre instâncias de grupos diferentes.

Por fim, na etapa de pós-processamento avalia-se se os padrões descobertos de
fato representam um \emph{conhecimento} novo sobre os dados. Para cada tipo
de padrão descoberto, pode-se estabelecer uma \emph{medida objetiva} sobre a
qualidade do padrão \cite{han2011data}. No caso dos agrupamentos, por exemplo,
a qualidade destes pode ser medida em termos de \emph{coesão} e \emph{separação}
\cite{tan2009introducao}.

Neste trabalho, será enfatizada a tarefa de agrupamento de dados, com atenção
especial aos algoritmos hierárquicos de agrupamento. Também será discutido como
as técnicas de redução de numerosidade influenciam o tempo de execução dos
algoritmos hierárquicos e a qualidade dos agrupamentos produzidos, como etapa de
pré-processamento.


\subsection{Técnicas de Amostragem de dados}
	\label{subsec:amostragem}
	
O objetivo das técnicas de amostragem de dados é reduzir o número de instâncias
submetidas aos algoritmos de mineração de dados. Entre os desafios da amostragem
de dados estão o balanceamento das instâncias com relação à ocorrência de
instâncias raras ou de exceções. Conside um conjunto $T$ com cardinalidade
$|T| = N$. Entre as técnicas propostas na literatura destacam-se
\cite{Garcia2015}:

\begin{itemize}
    \item Amostragem aleatória de tamanho $s$ sem substituição: criada pela
		escolha de $s$ instâncias de $T$ ($s < N$), onde a probabilidade de um
		exemplo ser escolhido é de $1/N$, de modo que todas as instâncias têm a
		mesma chance de serem escolhidas;
    
    \item Amostragem aleatória de tamanho $s$ com substituição: semelhante à
		anterior, exceto pelo fato que a cada vez que uma instância é escolhida,
		permanece no conjunto e pode ser escolhida novamente;
    
    \item Amostragem balanceada: criada levando-se em consideração um conjunto
		de critérios pré-definidos, por exemplo, para manter a proporcionalidade
		de instâncias entre classes conhecidas;
    
    \item Amostragem de agrupamentos: escolha de grupos específicos resultantes
		de técnicas de agrupamento;
    
    \item Amostragem estratificada: obtida por meio da divisão de um conjunto
		$T$ em partes mutualmente disjunta seguida da escolha de uma amostragem
		aleatória em cada divisão.
\end{itemize}

% Descrever as técnicas correlatas:
% Original Data Squashing (DS) e Likelihood-based Data Squashing (LDS).



\subsection{Análise de Agrupamentos}
	\label{subsec:analise_agrupamentos}
	
A análise de agrupamentos é uma tarefa de mineração de dados cujo objetivo é,
automaticamente, particionar o conjunto de dados em subconjuntos chamados
grupos. Os objetos reunidos em um mesmo grupo devem ser similares entre si,
enquanto que objetos de grupos separados devem ser diferentes. Ao conjunto dos
grupos resultantes da análise dá-se o nome de \emph{agrupamento}.

A análise de agrupamentos pode ser usada como uma ferramenta para extração de
conhecimento sobre um conjunto de dados ou então, como um etapa de
pré-processamento para outras tarefas de mineração de dados. Por exemplo, em
\cite{gonccalves2014land}, a análise de agrupamentos foi utilizada para
identificar o uso do terreno em diferentes regiões do estado de São Paulo,
Brasil. Já em \cite{petitjean2014dynamic}, a análise de agrupamentos foi
utilizada para eleger protótipos que posteriormente seriam utilizados como dados
de treinamento para a tarefa de classificação 1-NN.

Existem diversas abordagens para o agrupamento de dados. No agrupamento por
\emph{particionamento} o conjunto de dados é dividido em $k$ grupos, com cada 
grupo contendo pelo menos um objeto do conjunto. De maneira geral, estes
algoritmos consistem em: a partir de um agrupamento inicial, iterativamente 
realocar os objetos em grupos mais significativos até que um critério de parada
seja atingido. Podemos incluir nesta categoria os algoritmos \emph{k-médias} e
\emph{k-medoids}.

Uma abordagem alternativa é o agrupamento \emph{hierárquico}. Nesta abordagem, 
os objetos são organizados em uma hierarquia de grupos. Por sua vez, esta
hierarquia pode ser construída por duas maneiras diferentes:
\emph{aglomerativa} e \emph{divisiva}.

Na abordagem aglomerativa cada objeto de dados é inicialmente incluído em seu
próprio grupo. Em seguida, cada grupo é aglomerado com o seu grupo mais próximo,
formando uma relação "pai-filho" entre o grupo resultante e os grupos menores.
Esse processo se repete até que um único grupo, que contenha todos os dados do 
conjunto, seja obtido. Já na abordagem divisiva o processo se inverte. Todos os
objetos de dados são agrupados em um único grupo inicial, que será a raiz da 
hierarquia. Por sua vez, este grupo inicial é sucessivamente dividido em grupos
menores, até que cada objeto esteja em seu próprio grupo.

Na subseção \ref{subsec:abordagem_hierarquica} a abordagem hierárquica será
explorada com mais detalhes.

\subsection{Abordagem Hierárquica para Agrupamentos}
	\label{subsec:abordagem_hierarquica}
	
Na abordagem hierárquica de agrupamento, os grupos são organizados em árvore,
de forma as instâncias de dados são as folhas e a raiz é o grupo que representa
o conjunto de dados completo.  

% introduzir as tecnicas de agrupamento hierarquico: divisivo versus
% aglomerativo

% 29/05:
% apresentar os algoritmos single-link, complete-link, birch e dbscan

% explicar por que mesmo que o birch e o dbscan nao podem ser usados?


\subsection{Fractais e a propriedade de auto-similaridade}
	\label{subsec:fractais}

Um fractal pode ser definido pelo conceito de auto similaridade, no qual partes
de qualquer tamanho de um fractal são similares (exata ou estatisticamente) ao
conjunto todo. Um exemplo clássico de um fractal criado por meio da construção
repetitiva é o triângulo de Sierpinski, construído por meio de um processo
iterativo, onde se retira de um triângulo o triângulo central e para cada
triângulo resultante realiza-se o mesmo processo, recursivamente, conforme
apresentado na Figura \ref{fig:constrsierpinski}. O triângulo de Sierpinski
apresenta características interessantes, como o fato de cada triângulo interior
ser uma miniatura do triângulo em que está inserido, perímetro tendendo ao
infinito e área tendendo a zero quando o número de iterações tende ao infinito
\cite{Schroeder91}. 

\figura{sections/sierpinski.png}{0.3}
{Construção do triângulo de Sierpinski \cite{Schroeder91}.}
{fig:constrsierpinski}

O conceito de auto similaridade está relacionado com periodicidade. Muitos
fenômenos naturais e humanos acontecem com periodicidade. São exemplos o uso do
solo pela agricultura, o valor das moedas e das ações, o comportamento de redes
de comunicação, as filas de caixa de supermercado, o uso das estradas, as
músicas que tocam nas rádios e os efeitos da economia na vida das pessoas. Uma
razão para a universalidade dessas movimentações harmônicas é a linearidade
aproximada de muitos sistemas e a sua invariância com deslocamento no espaço e
tempo. Experimentos realizados com alguns conjuntos de dados sintéticos e reais
mostram que os dados referentes aos fenômenos humanos caracterizam-se por
apresentarem uma distribuição fractal \cite{Traina2010}.

Para a análise da auto similaridade de conjuntos contendo fenômenos naturais e
humanos, chamados de fractais estatisticamente auto-similares, utiliza-se o
método \textit{Box-Counting}. Para encontrar o \textit{Box-Counting} de um
conjunto de dados imerso em um espaço $E$-dimensional, deve-se dividir esse
espaço em células de um hipercubo de lado $r$, recursivamente, até encontrar um
elemento por célula \cite{Traina2010}. O método foi proposto para cálculo da
dimensão fractal, que corresponde ao número mínimo de dimensões para
representação de um conjunto (dimensionalidade intrinseca). 

Ainda preciso escrever sobre box plot com base em \cite{Traina2010}.
